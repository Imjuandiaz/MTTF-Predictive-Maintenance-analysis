# -*- coding: utf-8 -*-
"""MTBF_Predictive_Maintenance - OCT 2025

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cr6uKY3m00gCulunUnILpbXWMKlnEudv

##**MTBF Predictive Maintenance**
#Autor: Juan Diaz


#Importing Packages For ML Predictive Analysis
the objective of this project is to determinate the time [MIN] before the next machine failure
"""

#code snippet #1
#installing dython for correlation analysis
!pip install dython

#installing pycaret to AI/ML model test
!pip install git+https://github.com/pycaret/pycaret.git


#importing tools for data analisys
import missingno as msno
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""#Importing DataSet For ML Predictive Analysis"""

#code snippet 2
#reading and displaying data with pandas
data = pd.read_csv ('https://raw.githubusercontent.com/Imjuandiaz/MTTF-Predictive-Maintenance-analysis/refs/heads/main/Data/ai4i2020.csv')

data

"""#Data Info

"""

#code snippet 3
#show data columns
data.columns

#code snippet 3.5
#displaying data type
data.dtypes

"""#Proportion of Missing Value In the Data"""

#code snippet 4
#displaying the proportion of missing data in the dataset
msno.bar(data)

"""#Statistical Analysis"""

#code snippet 5
#displaying statistical analisys
data.describe()

"""#HeatMap Correlation Analysis With Dython"""

#code snippet 6
#import dython for correlacion analisys
import dython
from dython.nominal import associations
corr = associations(data)

#code snippet 7
# Storing the correlations in a matrix
assoc_matrix = corr['corr']

#Setting the target variable DonorIndicator
target = 'Machine failure'

# List the associated variables in the descending order of influence on the target variable
assoc_matrix[target].abs().sort_values(ascending=False)

"""#Featuring Engeenering"""

#CodeSnippet 8
# Torque distribution depending on failure
plt.figure(figsize=(8,5))
sns.boxplot(x='Machine failure', y='Torque [Nm]', data=data)
plt.title('Torque Distribution vs Machine Failure')
plt.show()

#CodeSnippet 9
# Creating new derived features
data['Temp_diff'] = data['Process temperature [K]'] - data['Air temperature [K]']
data['Wear_per_torque'] = data['Tool wear [min]'] / data['Torque [Nm]']

# Displaying first rows of engineered features
data[['Temp_diff', 'Wear_per_torque']].head()

"""#Pycaret For ML Model Testing"""

#code snippet 11
#setting up the Regression ML model
from pycaret.regression import *

#code snippet 12
#name of columns for AI/ML
data.dtypes

# Code snippet 17
# Setting the target variable
y = 'Tool wear [min]'

# Code snippet 18
# Setting the ignored variables(noise variables, redundant variables,  variables that are missing more than 20% data etc)
ignored_cols = ['Product ID','Machine failure', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF','UDI'
        ]

# Code snippet 19
# Setting the numerical variables
num_cols = [ 'Air temperature [K]',
       'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]',
       'Temp_diff', 'Wear_per_torque' ]

# Code snippet 20
# Setting the categorical variables
cat_cols = [ 'Type', ]

#CodeSnippet 21
# Setting up/configuring the pycaret setup for Regression ML modeling
from pycaret.regression import *

reg_setup = setup(
    data = data,
    target = y,        # o 'RUL_min' si implementas el target de tiempo restante
    train_size = 0.7,
    fold=10,
    ignore_features = ignored_cols,
    numeric_features = num_cols,
    categorical_features = cat_cols,
    verbose=False
)

# Code snippet 21
# Setting up/configuring the pycaret setup for Regression ML modeling
#Regression_setup = setup(data,
                       # target = y,
                        #ignore_features = ignored_cols,
                        #categorical_features = cat_cols,
                        #numeric_features = num_cols, train_size = 0.7)

"""#Compare ML Model"""

#code snippet 22
# Running pycaret to invoke ML algorithms
best = compare_models(sort='MAE')         # ExtraTrees/LightGBM suelen ser top
plot_model(best, plot='residuals')
plot_model(best, plot='error')
final_reg = finalize_model(best)
save_model(final_reg, 'model_time_pycaret')

"""#Predictions Dataset For Predictive Maintenance"""

# Code snippet 23
# Predicting 'eBook Subscriber Flag' on a dataset
predictions = predict_model(best, data)

# Code snippet 24
# Exporting predictions to a csv file
predictions.to_csv('Prediction maintenance predictions.csv')

"""#ML with Sklearn

"""

# CodeSnippet 25
# Selecting independent variables (features)
features = ['Air temperature [K]', 'Process temperature [K]',
            'Rotational speed [rpm]', 'Torque [Nm]',
            'Temp_diff', 'Wear_per_torque']

X = data[features]
y = data['Tool wear [min]']

# CodeSnippet 26
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Split 70/30 as in PyCaret
X_train, X_test, y_train, y_test = train_test_split(
    X, y, train_size=0.7, random_state=42
)

# Numeric and categorical columns
num_cols = ['Air temperature [K]', 'Process temperature [K]',
            'Rotational speed [rpm]', 'Torque [Nm]',
            'Temp_diff', 'Wear_per_torque']


# Column transformer for preprocessing
preprocessor = ColumnTransformer(transformers=[
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
])

#CodeSnippet 27 (Optimized RandomForest)
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, cross_val_score
import numpy as np

# Build optimized pipeline
rf_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', RandomForestRegressor(
        n_estimators=120,
        max_depth=12,
        min_samples_leaf=4,
        random_state=42,
        n_jobs=-1
    ))
])

# 10-Fold Cross Validation
cv = KFold(n_splits=10, shuffle=True, random_state=42)
cv_scores = cross_val_score(rf_pipeline, X_train, y_train,
                            cv=cv, scoring='neg_mean_absolute_error')

print(f"‚úÖ Cross-validation MAE: {abs(cv_scores.mean()):.3f} ¬± {cv_scores.std():.3f}")

# CodeSnippet 28
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Train final model
rf_pipeline.fit(X_train, y_train)

# Predict on test set
y_pred = rf_pipeline.predict(X_test)

# Evaluate performance
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"‚úÖ Test MAE:  {mae:.3f}")
print(f"‚úÖ Test RMSE: {rmse:.3f}")
print(f"‚úÖ Test R¬≤:   {r2:.3f}")

# CodeSnippet 29
import pandas as pd

# Extract trained model
rf_model = rf_pipeline.named_steps['model']

# Get processed feature names
encoded_num = preprocessor.named_transformers_['num'].get_feature_names_out(num_cols)
# Get encoded categorical feature names
feature_names = list(encoded_num)


# Feature importances
importances = pd.Series(rf_model.feature_importances_, index=feature_names)
importances = importances.sort_values(ascending=False)

print("üìä Top 10 Feature Importances:")
print(importances.head(10))

#CodeSnippet 30
import joblib, json
from pathlib import Path

ARTIFACTS_DIR = Path("artifacts")
ARTIFACTS_DIR.mkdir(exist_ok=True)

# 1Ô∏è‚É£ Save trained model with compression (native, no lz4)
joblib.dump(rf_pipeline, ARTIFACTS_DIR / "model_mtbf_randomforest.pkl", compress=('zlib', 3))

# 2Ô∏è‚É£ Save feature list
with open(ARTIFACTS_DIR / "features_mtbf.json", "w") as f:
    json.dump(features, f)

# 3Ô∏è‚É£ Save feature importances
importances.to_csv(ARTIFACTS_DIR / "feature_importances_mtbf.csv", header=["importance"])

print("‚úÖ Artifacts saved in ./artifacts:")
print("   - model_mtbf_randomforest.pkl (compressed)")
print("   - features_mtbf.json")
print("   - feature_importances_mtbf.csv")